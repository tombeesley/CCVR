@Article{chun1998,
  title = {Contextual {{Cueing}}: {{Implicit Learning}} and {{Memory}} of {{Visual Context Guides Spatial Attention}}},
  shorttitle = {Contextual {{Cueing}}},
  author = {Marvin M. Chun and Yuhong Jiang},
  year = {1998},
  month = {jun},
  volume = {36},
  pages = {28--71},
  issn = {00100285},
  doi = {10.1006/cogp.1998.0681},
  file = {D\:\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Chun, Jiang - 1998.pdf},
  journal = {Cognitive Psychology},
  language = {en},
  number = {1},
}
@Article{vadillo2016,
  title = {Underpowered Samples, False Negatives, and Unconscious Learning},
  author = {Miguel A. Vadillo and Emmanouil Konstantinidis and David R. Shanks},
  year = {2016},
  month = {feb},
  volume = {23},
  pages = {87--102},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-015-0892-6},
  file = {D\:\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Vadillo, Konstantinidis, Shanks - 2016.pdf},
  journal = {Psychonomic Bulletin \& Review},
  language = {en},
  number = {1},
}
@Article{colagiuri2016,
  title = {Contextual Cuing as a Form of Nonconscious Learning: {{Theoretical}} and Empirical Analysis in Large and Very Large Samples},
  shorttitle = {Contextual Cuing as a Form of Nonconscious Learning},
  author = {Ben Colagiuri and E. J. Livesey},
  year = {2016},
  month = {dec},
  volume = {23},
  pages = {1996--2009},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-016-1063-0},
  file = {D\:\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Colagiuri, Livesey - 2016.pdf},
  journal = {Psychonomic Bulletin \& Review},
  language = {en},
  number = {6},
}
@Article{vadillo2016,
  title = {Underpowered Samples, False Negatives, and Unconscious Learning},
  author = {Miguel A. Vadillo and Emmanouil Konstantinidis and David R. Shanks},
  year = {2016},
  month = {feb},
  volume = {23},
  pages = {87--102},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-015-0892-6},
  file = {D\:\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Vadillo, Konstantinidis, Shanks - 2016.pdf},
  journal = {Psychonomic Bulletin \& Review},
  language = {en},
  number = {1},
}
@Manual{R-base,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2020},
  url = {https://www.R-project.org/},
}
@Manual{R-citr,
  title = {citr: 'RStudio' Add-in to Insert Markdown Citations},
  author = {Frederik Aust},
  year = {2019},
  note = {R package version 0.3.2},
  url = {https://CRAN.R-project.org/package=citr},
}
@Manual{R-dplyr,
  title = {dplyr: A Grammar of Data Manipulation},
  author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller},
  year = {2020},
  note = {R package version 1.0.1},
  url = {https://CRAN.R-project.org/package=dplyr},
}
@Manual{R-forcats,
  title = {forcats: Tools for Working with Categorical Variables (Factors)},
  author = {Hadley Wickham},
  year = {2020},
  note = {R package version 0.5.0},
  url = {https://CRAN.R-project.org/package=forcats},
}
@Book{R-ggplot2,
  author = {Hadley Wickham},
  title = {ggplot2: Elegant Graphics for Data Analysis},
  publisher = {Springer-Verlag New York},
  year = {2016},
  isbn = {978-3-319-24277-4},
  url = {https://ggplot2.tidyverse.org},
}
@Manual{R-papaja,
  author = {Frederik Aust and Marius Barth},
  title = {{papaja}: {Create} {APA} manuscripts with {R Markdown}},
  year = {2020},
  note = {R package version 0.1.0.9997},
  url = {https://github.com/crsh/papaja},
}
@Manual{R-purrr,
  title = {purrr: Functional Programming Tools},
  author = {Lionel Henry and Hadley Wickham},
  year = {2020},
  note = {R package version 0.3.4},
  url = {https://CRAN.R-project.org/package=purrr},
}
@Manual{R-readr,
  title = {readr: Read Rectangular Text Data},
  author = {Hadley Wickham and Jim Hester and Romain Francois},
  year = {2018},
  note = {R package version 1.3.1},
  url = {https://CRAN.R-project.org/package=readr},
}
@Manual{R-stringr,
  title = {stringr: Simple, Consistent Wrappers for Common String Operations},
  author = {Hadley Wickham},
  year = {2019},
  note = {R package version 1.4.0},
  url = {https://CRAN.R-project.org/package=stringr},
}
@Manual{R-tibble,
  title = {tibble: Simple Data Frames},
  author = {Kirill Müller and Hadley Wickham},
  year = {2020},
  note = {R package version 3.0.3},
  url = {https://CRAN.R-project.org/package=tibble},
}
@Manual{R-tidyr,
  title = {tidyr: Tidy Messy Data},
  author = {Hadley Wickham and Lionel Henry},
  year = {2020},
  note = {R package version 1.1.1},
  url = {https://CRAN.R-project.org/package=tidyr},
}
@Article{R-tidyverse,
  title = {Welcome to the {tidyverse}},
  author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
  year = {2019},
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {43},
  pages = {1686},
  doi = {10.21105/joss.01686},
}
@Article{beesley2018,
  title = {Overt Attention in Contextual Cuing of Visual Search Is Driven by the Attentional Set, but Not by the Predictiveness of Distractors.},
  author = {Tom Beesley and Gunadi Hanafi and Miguel A. Vadillo and David. R. Shanks and Evan J. Livesey},
  year = {2018},
  month = {may},
  volume = {44},
  pages = {707--721},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/xlm0000467},
  abstract = {Two experiments examined biases in selective attention during contextual cuing of visual search. When participants were instructed to search for a target of a particular color, overt attention (as measured by the location of fixations) was biased strongly toward distractors presented in that same color. However, when participants searched for targets that could be presented in 1 of 2 possible colors, overt attention was not biased between the different distractors, regardless of whether these distractors predicted the location of the target (repeating) or did not (randomly arranged). These data suggest that selective attention in visual search is guided only by the demands of the target detection task (the attentional set) and not by the predictive validity of the distractor elements.},
  file = {D\:\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Beesley, Hanafi, Vadillo et al - 2018.pdf},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  language = {en},
  number = {5},
}
@Article{brady2007,
  title = {Spatial Constraints on Learning in Visual Search: {{Modeling}} Contextual Cuing.},
  shorttitle = {Spatial Constraints on Learning in Visual Search},
  author = {Timothy F. Brady and Marvin M. Chun},
  year = {2007},
  volume = {33},
  pages = {798--815},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/0096-1523.33.4.798},
  abstract = {Predictive visual context facilitates visual search, a benefit termed contextual cuing (M. M. Chun \& Y. Jiang, 1998). In the original task, search arrays were repeated across blocks such that the spatial configuration (context) of all of the distractors in a display predicted an embedded target location. The authors modeled existing results using a connectionist architecture and then designed new behavioral experiments to test the model's assumptions. The modeling and behavioral results indicate that learning may be restricted to the local context even when the entire configuration is predictive of target location. Local learning constrains how much guidance is produced by contextual cuing. The modeling and new data also demonstrate that local learning requires that the local context maintain its location in the overall global context.},
  file = {D\:\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Brady, Chun - 2007.pdf},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  language = {en},
  number = {4},
}
@Article{olson2002,
  title = {Perceptual Constraints on Implicit Learning of Spatial Context},
  author = {Ingrid R. Olson and Marvin M. Chun},
  year = {2002},
  month = {apr},
  volume = {9},
  pages = {273--302},
  issn = {1350-6285, 1464-0716},
  doi = {10.1080/13506280042000162},
  file = {D\:\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Olson, Chun - 2002.pdf},
  journal = {Visual Cognition},
  language = {en},
  number = {3},
}
@Article{beesley2015b,
  title = {Pre-Exposure of Repeated Search Configurations Facilitates Subsequent Contextual Cuing of Visual Search.},
  author = {Tom Beesley and Miguel A. Vadillo and Daniel Pearson and David R. Shanks},
  year = {2015},
  volume = {41},
  pages = {348--362},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/xlm0000033},
  abstract = {Contextual cuing is the enhancement of visual search when the configuration of distractors has been experienced previously. It has been suggested that contextual cuing relies on associative learning between the distractor locations and the target position. Four experiments examined the effect of pre-exposing configurations of consistent distractors on subsequent contextual cuing. The findings demonstrate a facilitation of subsequent cuing for pre-exposed configurations compared to novel configurations that have not been pre-exposed. This facilitation suggests that learning of repeated visual search patterns involves acquisition of not just distractor\textendash target associations but also associations between distractors within the search context, an effect that is not captured by the Brady and Chun (2007) connectionist model of contextual cuing. We propose a new connectionist model of contextual cuing that learns associations between repeated distractor stimuli, enabling it to predict an effect of pre-exposure on contextual cuing.},
  file = {D\:\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Beesley, Vadillo, Pearson et al - 2015.pdf},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  language = {en},
  number = {2},
}
@Article{beesley2016,
  title = {Configural Learning in Contextual Cuing of Visual Search.},
  author = {Tom Beesley and Miguel A. Vadillo and Daniel Pearson and David R. Shanks},
  year = {2016},
  volume = {42},
  pages = {1173--1185},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/xhp0000185},
  abstract = {Two experiments were conducted to explore the role of configural representations in contextual cuing of visual search. Repeating patterns of distractors (contexts) were trained incidentally as predictive of the target location. Training participants with repeating contexts of consistent configurations led to stronger contextual cuing than when participants were trained with contexts of inconsistent configurations. Computational simulations with an elemental associative learning model of contextual cuing demonstrated that purely elemental representations could not account for the results. However, a configural model of associative learning was able to simulate the ordinal pattern of data.},
  file = {D\:\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Beesley, Vadillo, Pearson et al - 2016.pdf},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  language = {en},
  number = {8},
}
@Article{kawahara2003,
  title = {Contextual Cueing in {{3D}} Layouts Defined by Binocular Disparity},
  author = {Jun-ichiro Kawahara},
  year = {2003},
  month = {oct},
  volume = {10},
  pages = {837--852},
  issn = {1350-6285, 1464-0716},
  doi = {10.1080/13506280344000103},
  file = {D\:\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Kawahara - 2003.pdf;D\:\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Kawahara - 22.pdf},
  journal = {Visual Cognition},
  language = {en},
  number = {7},
}
@Article{mcsorley2001,
  title = {Visual Search in Depth},
  author = {E. McSorley and J. M. Findlay},
  year = {2001},
  volume = {41},
  pages = {3487--3496},
  issn = {0042-6989},
  doi = {10.1016/s0042-6989(01)00197-3},
  abstract = {The accuracy of saccade localisation during visual search was examined for a search target defined by the single features of orientation or depth or by a conjunction of the two features. Subjects were required to move their eyes to the target which appeared in one of eight possible locations, arranged circularly around fixation, with non-targets filling the remaining seven positions. Search for a target defined by a single feature resulted in approximately 70\% correct first saccades in both cases, while search for the conjunction target resulted in only 40\% correct first saccades. Furthermore, averaged latency for conjunction search was longer than for simple search. Nevertheless, some subjects showed a remarkably good ability to locate a conjunction target with a single saccade of short latency. An analysis of first saccades in terms of their speed and accuracy indicates that the target selection is not preceded by a covert scanning of the display but rather is a result of parallel processing of the visual information provided. We also relate our study to the study of conjunction search reported by Nakayama and Siverman [Nakayama, K., \& Silverman, G.H. (1986). Serial and parallel processing of visual feature conjunctions. Nature, 320, 264-265.].},
  file = {D\:\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\McSorley, Findlay - 2001.pdf},
  journal = {Vision Research},
  keywords = {Adult,Analysis of Variance,Depth Perception,Female,Humans,Male,Orientation,Reaction Time,Saccades,Visual Pathways},
  language = {eng},
  number = {25-26},
  pmid = {11718789},
}

@Article{nakayama1986,
  title = {Serial and Parallel Processing of Visual Feature Conjunctions},
  author = {K. Nakayama and G. H. Silverman},
  year = {1986},
  month = {mar},
  volume = {320},
  pages = {264--265},
  issn = {0028-0836},
  doi = {10.1038/320264a0},
  abstract = {Treisman and others have reported that the visual search for a target distinguished along a single stimulus dimension (for example, colour or shape) is conducted in parallel, whereas the search for an item defined by the conjunction of two stimulus dimensions is conducted serially. For a single dimension the target 'pops out' and the search time is independent of the number of irrelevant items in the set. For conjunctions, the search time increases as the set becomes larger. Thus, it seems that the visual system is incapable of conducting a parallel search over two stimulus dimensions simultaneously. Here we extend this conclusion for the conjunction of motion and colour, showing that it requires a serial search. We also report two exceptions: if one of the dimensions in a conjunctive search is stereoscopic disparity, a second dimension of either colour or motion can be searched in parallel.},
  journal = {Nature},
  keywords = {Color,Depth Perception,Humans,Motion,Reaction Time,Vision; Ocular,Visual Cortex},
  language = {eng},
  number = {6059},
  pmid = {3960106},
}
@Article{zang2017,
  title = {Contextual Cueing in {{3D}} Visual Search Depends on Representations in Planar-, Not Depth-Defined Space},
  author = {Xuelian Zang and Zhuanghua Shi and Hermann J. M{\"u}ller and Markus Conci},
  year = {2017},
  month = {jun},
  volume = {17},
  pages = {17},
  issn = {1534-7362},
  doi = {10.1167/17.5.17},
  file = {D\:\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Zang, Shi, MÃ¼ller et al - 2017.pdf},
  journal = {Journal of Vision},
  language = {en},
  number = {5},
}
@Article{marek2020,
  title = {Contextual-{{Cueing}} beyond the {{Initial Field}} of {{View}}-{{A Virtual Reality Experiment}}},
  author = {Nico Marek and Stefan Pollmann},
  year = {2020},
  month = {jul},
  volume = {10},
  issn = {2076-3425},
  doi = {10.3390/brainsci10070446},
  abstract = {In visual search, participants can incidentally learn spatial target-distractor configurations, leading to shorter search times for repeated compared to novel configurations. Usually, this is tested within the limited visual field provided by a computer monitor. While contextual cueing is typically investigated on two-dimensional screens, we present for the first time an implementation of a classic contextual cueing task (search for a T-shape among L-shapes) in a three-dimensional virtual environment. This enabled us to test if the typical finding of incidental learning of repeated search configurations, manifested by shorter search times, would hold in a three-dimensional virtual reality (VR) environment. One specific aspect that was tested by combining virtual reality and contextual cueing was if contextual cueing would hold for targets outside the initial field of view (FOV), requiring head movements to be found. In keeping with two-dimensional search studies, reduced search times were observed after the first epoch and remained stable in the remaining experiment. Importantly, comparable search time reductions were observed for targets both within and outside of the initial FOV. The results show that a repeated distractors-only configuration in the initial FOV can guide search for target locations requiring a head movement to be seen.},
  file = {D\:\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Marek, Pollmann - 2020.pdf},
  journal = {Brain Sciences},
  keywords = {contextual cueing,virtual reality,visual search},
  language = {eng},
  number = {7},
  pmid = {32668806},
}
@Manual{R-shiny,
  title = {shiny: Web Application Framework for R},
  author = {Winston Chang and Joe Cheng and JJ Allaire and Yihui Xie and Jonathan McPherson},
  year = {2020},
  note = {R package version 1.5.0},
  url = {https://CRAN.R-project.org/package=shiny},
}
@Manual{R-english,
  title = {english: Translate Integers into English},
  author = {John Fox and Bill Venables and Anthony Damico and Anne Pier Salverda},
  year = {2020},
  note = {R package version 1.2-5},
  url = {https://CRAN.R-project.org/package=english},
}
@Manual{R-extrafont,
  title = {extrafont: Tools for using fonts},
  author = {Winston Chang,},
  year = {2014},
  note = {R package version 0.17},
  url = {https://CRAN.R-project.org/package=extrafont},
}
@Manual{R-afex,
  title = {afex: Analysis of Factorial Experiments},
  author = {Henrik Singmann and Ben Bolker and Jake Westfall and Frederik Aust and Mattan S. Ben-Shachar},
  year = {2020},
  note = {R package version 0.27-2},
  url = {https://CRAN.R-project.org/package=afex},
}
@Manual{R-apa,
  title = {apa: Format Outputs of Statistical Tests According to APA Guidelines},
  author = {Daniel Gromer},
  year = {2020},
  note = {R package version 0.3.3},
  url = {https://CRAN.R-project.org/package=apa},
}
@Article{R-lme4,
  title = {Fitting Linear Mixed-Effects Models Using {lme4}},
  author = {Douglas Bates and Martin M{\"a}chler and Ben Bolker and Steve Walker},
  journal = {Journal of Statistical Software},
  year = {2015},
  volume = {67},
  number = {1},
  pages = {1--48},
  doi = {10.18637/jss.v067.i01},
}
@Manual{R-Matrix,
  title = {Matrix: Sparse and Dense Matrix Classes and Methods},
  author = {Douglas Bates and Martin Maechler},
  year = {2019},
  note = {R package version 1.2-18},
  url = {https://CRAN.R-project.org/package=Matrix},
}
@Manual{R-pwr,
  title = {pwr: Basic Functions for Power Analysis},
  author = {Stephane Champely},
  year = {2020},
  note = {R package version 1.3-0},
  url = {https://CRAN.R-project.org/package=pwr},
}
@Manual{R-effsize,
  title = {effsize: Efficient Effect Size Computation},
  author = {Marco Torchiano},
  year = {2020},
  note = {R package version 0.8.0},
  doi = {10.5281/zenodo.1480624},
  url = {https://CRAN.R-project.org/package=effsize},
}
@Manual{R-lsr,
  title = {Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.5)},
  author = {Daniel Navarro},
  organization = {University of Adelaide},
  address = {Adelaide, Australia},
  year = {2015},
  note = {R package version 0.5},
  url = {http://ua.edu.au/ccs/teaching/lsr},
}
@Article{smyth2008,
  title = {Awareness in Contextual Cuing with Extended and Concurrent Explicit Tests},
  author = {Andrea C. Smyth and David R. Shanks},
  year = {2008},
  month = {mar},
  volume = {36},
  pages = {403--415},
  issn = {0090-502X, 1532-5946},
  doi = {10.3758/MC.36.2.403},
  file = {D\:\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Smyth, Shanks - 2008.pdf},
  journal = {Memory \& Cognition},
  language = {en},
  number = {2},
}

@article{zang2017,
	title = {Contextual cueing in 3D visual search depends on representations in planar-, not depth-defined space},
	author = {{Zang}, {Xuelian} and {Shi}, {Zhuanghua} and {Müller}, {Hermann J.} and {Conci}, {Markus}},
	year = {2017},
	month = {06},
	date = {2017-06-12},
	journal = {Journal of Vision},
	pages = {17},
	volume = {17},
	number = {5},
	doi = {10.1167/17.5.17},
	url = {http://jov.arvojournals.org/article.aspx?doi=10.1167/17.5.17},
	langid = {en}
}
@Manual{R-skimr,
  title = {skimr: Compact and Flexible Summaries of Data},
  author = {Elin Waring and Michael Quinn and Amelia McNamara and Eduardo {Arino de la Rubia} and Hao Zhu and Shannon Ellis},
  year = {2021},
  note = {R package version 2.1.3},
  url = {https://CRAN.R-project.org/package=skimr},
}
@Manual{R-broom,
  title = {broom: Convert Statistical Objects into Tidy Tibbles},
  author = {David Robinson and Alex Hayes and Simon Couch},
  year = {2021},
  note = {R package version 0.7.4},
  url = {https://CRAN.R-project.org/package=broom},
}
@Manual{R-kableExtra,
  title = {kableExtra: Construct Complex Table with 'kable' and Pipe Syntax},
  author = {Hao Zhu},
  year = {2020},
  note = {R package version 1.3.1},
  url = {https://CRAN.R-project.org/package=kableExtra},
}
@Book{R-knitr,
  title = {Dynamic Documents with {R} and knitr},
  author = {Yihui Xie},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2015},
  edition = {2nd},
  note = {ISBN 978-1498716963},
  url = {https://yihui.org/knitr/},
}
@Manual{R-patchwork,
  title = {patchwork: The Composer of Plots},
  author = {Thomas Lin Pedersen},
  year = {2020},
  note = {R package version 1.1.1},
  url = {https://CRAN.R-project.org/package=patchwork},
}
@Manual{R-BayesFactor,
  title = {BayesFactor: Computation of Bayes Factors for Common Designs},
  author = {Richard D. Morey and Jeffrey N. Rouder},
  year = {2018},
  note = {R package version 0.9.12-4.2},
  url = {https://CRAN.R-project.org/package=BayesFactor},
}
@Article{R-coda,
  title = {CODA: Convergence Diagnosis and Output Analysis for MCMC},
  author = {Martyn Plummer and Nicky Best and Kate Cowles and Karen Vines},
  journal = {R News},
  year = {2006},
  volume = {6},
  number = {1},
  pages = {7--11},
  url = {https://journal.r-project.org/archive/},
  pdf = {https://www.r-project.org/doc/Rnews/Rnews_2006-1.pdf},
}
@Manual{R-rstudioapi,
  title = {rstudioapi: Safely Access the RStudio API},
  author = {Kevin Ushey and JJ Allaire and Hadley Wickham and Gary Ritchie},
  year = {2020},
  note = {R package version 0.13},
  url = {https://CRAN.R-project.org/package=rstudioapi},
}
@Article{vadillo2021,
  title = {Raising Awareness about Measurement Error in Research on Unconscious Mental Processes},
  author = {Miguel A. Vadillo and Simone Malejka and Daryl Y. H. Lee and Zoltan Dienes and David R. Shanks},
  year = {2021},
  month = {jun},
  issn = {1531-5320},
  doi = {10.3758/s13423-021-01923-y},
  abstract = {Experimental psychologists often neglect the poor psychometric properties of the dependent measures collected in their studies. In particular, a low reliability of measures can have dramatic consequences for the interpretation of key findings in some of the most popular experimental paradigms, especially when strong inferences are drawn from the absence of statistically significant correlations. In research on unconscious cognition, for instance, it is commonly argued that the lack of a correlation between task performance and measures of awareness or explicit recollection of the target stimuli provides strong support for the conclusion that the cognitive processes underlying performance must be unconscious. Using contextual cuing of visual search as a case study, we show that given the low reliability of the dependent measures collected in these studies, it is usually impossible to draw any firm conclusion about the unconscious character of this effect from correlational analyses. Furthermore, both a psychometric meta-analysis of the available evidence and a cognitive-modeling approach suggest that, in fact, we should expect to see very low correlations between performance and awareness at the empirical level, even if both constructs are perfectly related at the latent level. Convincing evidence for the unconscious character of contextual cuing and other effects will most likely demand richer and larger data sets, coupled with more powerful analytic approaches.},
  file = {D\:\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Vadillo, Malejka, Lee et al - 2021.pdf},
  journal = {Psychonomic Bulletin \& Review},
  keywords = {Contextual cuing,Meta-analysis,Reliability,Unconscious learning},
  language = {eng},
  pmid = {34131891},
}
@Manual{R-ggrepel,
  title = {ggrepel: Automatically Position Non-Overlapping Text Labels with
'ggplot2'},
  author = {Kamil Slowikowski},
  year = {2021},
  note = {R package version 0.9.1},
  url = {https://CRAN.R-project.org/package=ggrepel},
}
@Article{cousineau2005,
  title = {Confidence Intervals in Within-Subject Designs: {{A}} Simpler Solution to {{Loftus}} and {{Masson}}'s Method},
  shorttitle = {Confidence Intervals in Within-Subject Designs},
  author = {Denis Cousineau},
  date = {2005-09-01},
  journaltitle = {Tutorials in Quantitative Methods for Psychology},
  shortjournal = {TQMP},
  volume = {1},
  number = {1},
  pages = {42--45},
  issn = {1913-4126},
  doi = {10.20982/tqmp.01.1.p042},
  url = {http://www.tqmp.org/RegularArticles/vol01-1/p042},
  urldate = {2019-08-16},
  langid = {english},
  file = {C\:\\Users\\beesleyt\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Cousineau - 2005.pdf},
}
@Article{kroellBehaviouralEvidenceSingle2019a,
  title = {Behavioural Evidence for a Single Memory System in Contextual Cueing},
  author = {Lisa M. Kroell and Bernhard Schlagbauer and Artyom Zinchenko and Hermann J. M{\~A}{\textflorin}{\^A}{\textonequarter}ller and Thomas Geyer},
  date = {2019},
  journaltitle = {Visual Cognition},
  volume = {27},
  number = {5-8},
  pages = {551--562},
  publisher = {{Taylor \& Francis}},
  location = {{United Kingdom}},
  issn = {1464-0716},
  doi = {10.1080/13506285.2019.1648347},
  abstract = {If a target is repeatedly encountered within a stable search array, target detection is accelerated over time. Nonetheless, participants fail to identify repeated search layouts in recognition tests. This dissociation has motivated the assumption that search and recognition performances are driven by two distinct memory components. Search facilitations are supported by an unconscious system, rendering memory traces inaccessible for report. The opposite is true for the conscious system: intentional learning of search layouts should result in lasting recognition. At the same time, explicitly acquired memory representations should be unable to generate search advantages in the absence of awareness. To test these assumptions, we introduced an intentional learning task in which participants memorized a set of â€œexplicitâ€ displays. During search, explicit displays appeared alongside repeated, yet not previously studied (â€œimplicitâ€) and new configurations. Explicit displays elicited lower reaction times, fewer fixations, and a more efficient scan path compared to implicit arrays. Using a statistically powerful recognition test, we demonstrate above-chance recognition of explicit and implicit displays. We consequently suggest that all information is stored in a single memory system, with the strength of representations varying from weak to high. Across the continuum, contents are accessible for retrieval during search and recognition. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Awareness,Contextual Cues,Dissociation,Intentional Learning,Memory,Reaction Time,Visual Search},
  file = {C\:\\Users\\beesleyt\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Kroell, Schlagbauer, Zinchenko et al - 2019.pdf},
}
@Article{kroell2019,
  title = {Behavioural Evidence for a Single Memory System in Contextual Cueing},
  author = {Lisa M. Kroell and Bernhard Schlagbauer and Artyom Zinchenko and Hermann J. M{\~A}{\textflorin}{\^A}{\textonequarter}ller and Thomas Geyer},
  date = {2019},
  journaltitle = {Visual Cognition},
  volume = {27},
  number = {5-8},
  pages = {551--562},
  publisher = {{Taylor \& Francis}},
  location = {{United Kingdom}},
  issn = {1464-0716},
  doi = {10.1080/13506285.2019.1648347},
  abstract = {If a target is repeatedly encountered within a stable search array, target detection is accelerated over time. Nonetheless, participants fail to identify repeated search layouts in recognition tests. This dissociation has motivated the assumption that search and recognition performances are driven by two distinct memory components. Search facilitations are supported by an unconscious system, rendering memory traces inaccessible for report. The opposite is true for the conscious system: intentional learning of search layouts should result in lasting recognition. At the same time, explicitly acquired memory representations should be unable to generate search advantages in the absence of awareness. To test these assumptions, we introduced an intentional learning task in which participants memorized a set of â€œexplicitâ€ displays. During search, explicit displays appeared alongside repeated, yet not previously studied (â€œimplicitâ€) and new configurations. Explicit displays elicited lower reaction times, fewer fixations, and a more efficient scan path compared to implicit arrays. Using a statistically powerful recognition test, we demonstrate above-chance recognition of explicit and implicit displays. We consequently suggest that all information is stored in a single memory system, with the strength of representations varying from weak to high. Across the continuum, contents are accessible for retrieval during search and recognition. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Awareness,Contextual Cues,Dissociation,Intentional Learning,Memory,Reaction Time,Visual Search},
  file = {C\:\\Users\\beesleyt\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Kroell, Schlagbauer, Zinchenko et al - 2019.pdf},
}
@Article{zheng2019,
  title = {The Contribution of Spatial Position and Rotated Global Configuration to Contextual Cueing},
  author = {Lei Zheng and Stefan Pollmann},
  date = {2019-11-01},
  journaltitle = {Attention, Perception, \& Psychophysics},
  shortjournal = {Atten Percept Psychophys},
  volume = {81},
  number = {8},
  pages = {2590--2596},
  issn = {1943-393X},
  doi = {10.3758/s13414-019-01871-9},
  url = {https://doi.org/10.3758/s13414-019-01871-9},
  urldate = {2022-03-11},
  abstract = {Spatial information can incidentally guide attention to the likely location of a target. This contextual cueing was even observed if only the relative configuration, but not the individual locations of distractor items were repeated or vice versa (Jiang \& Wagner in Perception \& Psychophysics, 66(3), 454-463, 2004). The present study investigated the contribution of global configuration and individual spatial location to contextual cueing. Participants repeatedly searched 12 visual search displays in a learning session. In a subsequent transfer session, there were four conditions: fully repeated configurations (same as the displays in the learning session), recombined configurations from two learned configurations with the same target location (preserving distractor locations but not configuration), rotated configurations (preserving configuration but not distractor locations), and new configurations. We could show that contextual cueing occurred if only distractor locations or relative configuration, randomly intermixed, was preserved in a single experiment. Beyond replicating the results of Jiang and Wagner, we made an adjustment to a particular type of transformation â€“ that may have occurred in separate experiments â€“ unlikely. Moreover, contextual cueing in rotated configurations showed that repeated configurations can serve as context cues even without preserved azimuth.},
  langid = {english},
  file = {C\:\\Users\\beesleyt\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Zheng, Pollmann - 2019.pdf},
}
@Article{dienes2014,
  title = {Using {{Bayes}} to Get the Most out of Non-Significant Results},
  author = {Zoltan Dienes},
  date = {2014},
  journaltitle = {Frontiers in Psychology},
  volume = {5},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/article/10.3389/fpsyg.2014.00781},
  urldate = {2022-03-13},
  abstract = {No scientific conclusion follows automatically from a statistically non-significant result, yet people routinely use non-significant results to guide conclusions about the status of theories (or the effectiveness of practices). To know whether a non-significant result counts against a theory, or if it just indicates data insensitivity, researchers must use one of: power, intervals (such as confidence or credibility intervals), or else an indicator of the relative evidence for one theory over another, such as a Bayes factor. I argue Bayes factors allow theory to be linked to data in a way that overcomes the weaknesses of the other approaches. Specifically, Bayes factors use the data themselves to determine their sensitivity in distinguishing theories (unlike power), and they make use of those aspects of a theoryâ€™s predictions that are often easiest to specify (unlike power and intervals, which require specifying the minimal interesting value in order to address theory). Bayes factors provide a coherent approach to determining whether non-significant results support a null hypothesis over a theory, or whether the data are just insensitive. They allow accepting and rejecting the null hypothesis to be put on an equal footing. Concrete examples are provided to indicate the range of application of a simple online Bayes calculator, which reveal both the strengths and weaknesses of Bayes factors.},
  file = {C\:\\Users\\beesleyt\\Zotero\\storage\\P6ZTMZ7W\\Dienes - 2014.pdf},
}
